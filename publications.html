---
layout: default
title: Publications
---

<h1>Publications</h1>

<div class="publications">

  <div class="pub-year">2025</div>

  <div class="pub-entry">
    <div class="pub-info">
      <p class="pub-title">A dynamic likelihood approach to filtering transport processes: advection-diffusion dynamics</p>
      <p class="pub-authors"> <u>Johannes Krotz</u>,Juan M. Restrepo, Jorge Ramirez</p>
      <p class="pub-journal"><em>Journal of Computational Physics</em></p>
    </div>

    <div class="pub-buttons">
      <button class="pub-btn toggle-btn" data-target="abs-1">Abstract</button>
      <button class="pub-btn toggle-btn" data-target="bib-1">BibTeX</button>
      <a class="pub-btn" href="https://doi.org/10.1016/j.jcp.2025.114089" target="_blank">URL</a>
     <button class="pub-btn toggle-btn" data-target="sum-1">In Simple Terms</button>
    </div>

    <div class="pub-toggle hidden" id="abs-1">
      <p>
      A Bayesian data assimilation scheme is formulated for advection-dominated advective and diffusive evolutionary problems, based upon the Dynamic Likelihood (DLF) approach to filtering. The DLF was developed specifically for hyperbolic problems –waves–, and in this paper, it is extended via a split step formulation, to handle advection-diffusion problems. In the dynamic likelihood approach, observations and their statistics are used to propagate probabilities along characteristics, evolving the likelihood in time. The estimate posterior thus inherits phase information. For advection-diffusion the advective part of the time evolution is handled on the basis of observations alone, while the diffusive part is informed through the model as well as observations. We expect, and indeed show here, that in advection-dominated problems, the DLF approach produces better estimates than other assimilation approaches, particularly when the observations are sparse and have low uncertainty. The added computational expense of the method is cubic in the total number of observations over time, which is on the same order of magnitude as a standard Kalman filter and can be mitigated by bounding the number of forward propagated observations, discarding the least informative data.</p>
    </div>

    <div class="pub-toggle hidden" id="bib-1">
      <pre>@article{KROTZ_2025_DynamicLikelihoodFilter,
title = {A dynamic likelihood approach to filtering transport processes: advection-diffusion dynamics},
journal = {Journal of Computational Physics},
volume = {536},
pages = {114089},
year = {2025},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2025.114089},
url = {https://www.sciencedirect.com/science/article/pii/S0021999125003729},
author = {Johannes Krotz and Juan M. Restrepo and Jorge Ramirez},
keywords = {Data assimilation, Bayesian estimation, Dynamic likelihood filter, 
Advection-diffusion, Transport, Kalman filter},
abstract = {A Bayesian data assimilation scheme is formulated for advection-dominated advective 
        and diffusive evolutionary problems, based upon the Dynamic Likelihood (DLF) approach to 
        filtering. The DLF was developed specifically for hyperbolic problems –waves–, and in this
        paper, it is extended via a split step formulation, to handle advection-diffusion problems. 
        In the dynamic likelihood approach, observations and their statistics are used to propagate 
        probabilities along characteristics, evolving the likelihood in time. The estimate posterior
        thus inherits phase information. For advection-diffusion the advective part of the time 
        evolution is handled on the basis of observations alone, while the diffusive part is informed 
        through the model as well as observations. We expect, and indeed show here, that in 
        advection-dominated problems, the DLF approach produces better estimates than other 
        assimilation approaches, particularly when the observations are sparse and have low 
        uncertainty. The added computational expense of the method is cubic in the total number of 
        observations over time, which is on the same order of magnitude as a standard Kalman filter 
        and can be mitigated by bounding the number of forward propagated observations, discarding 
        the least informative data.}}}
        </pre>
    </div>
<div class="pub-toggle hidden" id="sum-1">
  <p>
    Imagine you’re trying to follow the spread of smoke in the air, or how pollutants drift and mix in a river. These
    processes are driven by two forces: <strong>advection</strong> (movement along currents, carrying things downstream
    or with the wind) and <strong>diffusion</strong> (spreading and blurring out over time). Scientists want to predict
    these processes as accurately as possible — but the problem is, we never have perfect information. We rely on
    <em>models</em> (our best guess of the physics) and <em>observations</em> (data from sensors). Blending these two
    is called <strong>data assimilation</strong>.
  </p>

  <p>
    The classic tool for this job is the <strong>Kalman Filter</strong>, which acts like a smart balance: constantly
    correcting a model with incoming data. It works well when observations are plentiful and evenly spaced. But in the
    real world, data is often <em>sparse</em> (only a few sensors, not everywhere all the time) — and this is exactly
    where traditional methods begin to struggle.
  </p>

  <h3>The Core Idea: A Dynamic Likelihood</h3>
  <p>
    This paper develops the <strong>Dynamic Likelihood Filter (DLF)</strong> for advection–diffusion problems. The key
    innovation is that instead of waiting passively for the next measurement, the DLF <em>projects existing observations
    forward in time</em> along the natural flow of the system. In other words, it doesn’t just use a measurement once and
    discard it. It “carries” that information along the path particles or pollutants would naturally follow, generating
    <strong>pseudo-observations</strong> at new times and places where sensors may not exist.
  </p>

  <p>
    For the <em>advective part</em> of the process (the bulk flow), these pseudo-observations are especially powerful.
    For the <em>diffusive part</em> (the spreading and smoothing), the DLF blends in predictions from the model itself.
    Together, this allows the filter to maintain not only the <em>amount</em> of what is being transported, but also its
    <em>location and phase</em> — something traditional methods tend to lose quickly as diffusion blurs the signal.
  </p>

  <h3>Why This Matters</h3>
  <p>
    The DLF is designed for the kinds of real-world challenges that matter most:
  </p>
  <ul>
    <li><strong>Sparse but reliable data:</strong> a few high-quality sensors instead of dense networks.</li>
    <li><strong>Advection-dominated systems:</strong> where currents, winds, or flows move things much more than they diffuse.</li>
    <li><strong>Uncertain models:</strong> situations where we know the equations but don’t capture every detail of reality.</li>
  </ul>
  <p>
    These situations arise in <strong>meteorology</strong> (weather forecasting with limited stations), <strong>environmental
    monitoring</strong> (tracking contaminants in rivers or the atmosphere), and <strong>engineering</strong> (any process
    where materials or signals are transported through space and time).
  </p>

  <h3>What the Results Show</h3>
  <p>
    Numerical experiments in the paper show that the DLF consistently outperforms the standard Kalman Filter when
    advection dominates and data is sparse. It:
  </p>
  <ul>
    <li>Produces more accurate estimates of both <em>intensity</em> and <em>location</em>.</li>
    <li>Corrects errors in initial conditions more quickly.</li>
    <li>Handles uncertain or slightly wrong models better, staying anchored to reality.</li>
  </ul>
  <p>
    There is a tradeoff: the DLF requires more computation. But the paper demonstrates practical ways to manage this,
    such as discarding “stale” pseudo-observations once they no longer add value. This keeps the method efficient while
    retaining its accuracy advantage.
  </p>

  <h3>The Bigger Picture</h3>
  <p>
    Originally, the Dynamic Likelihood idea was built for wave problems. Extending it to advection–diffusion, one of
    the most common forms of transport in nature and engineering,  shows its broader potential. By projecting
    observations dynamically, the DLF squeezes more information out of every measurement. It makes data assimilation
    <strong>smarter, more resilient to sparse data, and better at tracking where things actually are</strong>.
  </p>

  <p>
    <em>In short:</em> The Dynamic Likelihood Filter lets us do more with less. It turns limited observations into a
    continuous source of information, improving forecasts in fields from weather and climate science to pollution
    monitoring and engineering transport problems.
  </p>
</div>
  </div>

</div>



<div class="pub-year">2024</div>

<div class="pub-entry">
  <div class="pub-info">
    <p class="pub-title">A hybrid Monte Carlo, discontinuous Galerkin method for linear kinetic transport equations</p>
    <p class="pub-authors"><u>Johannes Krotz</u>, Cory D. Hauck, Ryan G. McClarren</p>
    <p class="pub-journal"><em>Journal of Computational Physics</em></p>
  </div>

  <div class="pub-buttons">
    <button class="pub-btn toggle-btn" data-target="abs-2">Abstract</button>
    <button class="pub-btn toggle-btn" data-target="bib-2">BibTeX</button>
    <a class="pub-btn" href="https://doi.org/10.1016/j.jcp.2024.113253" target="_blank">URL</a>
    <button class="pub-btn toggle-btn" data-target="sum-2">In Simple Terms</button>
  </div>

  <div class="pub-toggle hidden" id="abs-2">
    <p>We present a hybrid method for time-dependent particle transport problems that combines Monte Carlo (MC) estimation with deterministic solutions based on discrete ordinates. For spatial discretizations, the MC algorithm computes a piecewise constant solution and the discrete ordinates use bilinear discontinuous finite elements. From the hybridization of the problem, the resulting problem solved by Monte Carlo is scattering free, resulting in a simple, efficient solution procedure. Between time steps, we use a projection approach to “relabel” collided particles as uncollided particles. From a series of standard 2-D Cartesian test problems we observe that our hybrid method has improved accuracy and reduction in computational complexity of approximately an order of magnitude relative to standard discrete ordinates solutions.</p>
  </div>

  <div class="pub-toggle hidden" id="bib-2">
    <pre>@article{KROTZ_2024_HybridMCDG,
title = {A hybrid Monte Carlo, discontinuous Galerkin method for linear kinetic transport equations},
journal = {Journal of Computational Physics},
volume = {514},
pages = {113253},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2024.113253},
url = {https://www.sciencedirect.com/science/article/pii/S0021999124005011},
author = {Johannes Krotz and Cory D. Hauck and Ryan G. McClarren},
keywords = {Hybrid stochastic-deterministic method, Monte Carlo, Kinetic equations, Particle transport},
abstract = {We present a hybrid method for time-dependent particle transport problems that combines
         Monte Carlo (MC) estimation with deterministic solutions based on discrete ordinates. For 
         spatial discretizations, the MC algorithm computes a piecewise constant solution and the 
         discrete ordinates use bilinear discontinuous finite elements. From the hybridization of 
         the problem, the resulting problem solved by Monte Carlo is scattering free, resulting in 
         a simple, efficient solution procedure. Between time steps, we use a projection approach to 
         “relabel” collided particles as uncollided particles. From a series of standard 2-D Cartesian
         test problems we observe that our hybrid method has improved accuracy and reduction in 
         computational complexity of approximately an order of magnitude relative to standard discrete
         ordinates solutions.}}
    </pre>
  </div>

  <div class="pub-toggle hidden" id="sum-2">  
  <p>
    Imagine trying to predict how sunlight travels through Earth’s atmosphere, how radiation moves inside a nuclear reactor, 
    or how heat from a laser interacts with a dense plasma. All of these processes involve <strong>particles moving, bouncing, 
    and sometimes getting absorbed</strong> as they travel through a medium. Modeling these processes accurately is a monumental challenge. 
  </p>

  <p>
    Traditionally, scientists have relied on two approaches: <em>deterministic methods</em>, which translate physics into large systems 
    of equations, and <em>Monte Carlo methods</em>, which simulate particle paths randomly. Each has strengths and weaknesses. Deterministic 
    methods can be precise but become prohibitively expensive at high resolution. Monte Carlo is flexible and natural for complex geometries, 
    but it’s noisy and often requires an enormous number of samples to be reliable. 
  </p>

  <h3>The Core Idea: Pre- and Post-Collision Particles</h3>
  <p>
    The hybrid method presented in this paper blends the two approaches by recognizing that particles behave differently depending 
    on whether they’ve collided yet. 
  </p>
  <ul>
    <li><strong>Pre-collision (uncollided):</strong> particles stream like rays of light. Their paths are straightforward, and Monte Carlo 
    can handle them efficiently — essentially like ray tracing.</li>
    <li><strong>Post-collision (collided):</strong> once particles scatter, their distribution becomes smoother and more diffuse. A deterministic 
    solver is better suited here, capturing the averaged behavior without tracking every random path.</li>
  </ul>
  <p>
    A clever <em>relabeling step</em> keeps the balance by periodically shifting some particles back into the pre-collision group, ensuring 
    the method stays both accurate and efficient over time.
  </p>

  <h3>Results &amp; Takeaways</h3>
  <p>
    Across standard benchmark problems, the hybrid method consistently delivered <strong>greater accuracy at significantly lower cost</strong>. 
    Unlike pure deterministic solvers, it avoided artificial streaks (“ray effects”), and unlike pure Monte Carlo, it didn’t require massive 
    sample counts. Overall, it achieved comparable or better accuracy with <em>about an order of magnitude less computational complexity</em>.
  </p>
  <p>
    This is more than a technical curiosity, it has direct relevance to <strong>nuclear engineering</strong>, <strong>astrophysics</strong>, 
    <strong>medical physics</strong>, and <strong>fusion research</strong>. Anywhere particles stream and scatter through matter, this method 
    can provide faster, cleaner, and more reliable simulations. 
  </p>

  <h3>Looking Ahead</h3>
  <p>
    While the current work focuses on single-energy problems, the approach naturally extends to more complex cases, such as multi-energy 
    neutron transport or nonlinear radiative transfer in astrophysics. Future versions may even include adaptive strategies to automatically 
    balance the two solvers as the simulation evolves. 
  </p>

  <p>
    <em>In short:</em> By treating particles before and after collisions differently,  Monte Carlo for the sharp, ray-like stage, and deterministic 
    solvers for the scattered stage,  this hybrid method achieves what neither approach can do alone: <strong>faster, more accurate, 
    and more practical simulations of particle transport</strong>.
  </p>
  </div>
</div>




<div class="pub-year">2023</div>

<div class="pub-entry">
  <div class="pub-info">
    <p class="pub-title">Variable resolution Poisson-disk sampling for meshing discrete fracture networks</p>
    <p class="pub-authors"><u>Johannes Krotz</u>, Matthew R. Sweeney, Carl W. Gable, Jeffrey D. Hyman, Juan M. Restrepo </p>
    <p class="pub-journal"><em>Journal of Computational and Applied Mathematics</em></p>
  </div>

  <div class="pub-buttons">
    <button class="pub-btn toggle-btn" data-target="abs-3">Abstract</button>
    <button class="pub-btn toggle-btn" data-target="bib-3">BibTeX</button>
    <a class="pub-btn" href="https://doi.org/10.1016/j.cam.2022.114094" target="_blank">URL</a>
    <button class="pub-btn toggle-btn" data-target="sum-3">In Simple Terms</button>
  </div>

  <div class="pub-toggle hidden" id="abs-3">
    <p>We present the near-Maximal Algorithm for Poisson-disk Sampling (nMAPS) to generate point distributions for variable resolution Delaunay triangular and tetrahedral meshes in two and three-dimensions, respectively. nMAPS consists of two principal stages. In the first stage, an initial point distribution is produced using a cell-based rejection algorithm. In the second stage, holes in the sample are detected using an efficient background grid and filled in to obtain a near-maximal covering. Extensive testing shows that nMAPS generates a variable resolution mesh in linear run time with the number of accepted points. We demonstrate nMAPS capabilities by meshing three-dimensional discrete fracture networks (DFN) and the surrounding volume. The discretized boundaries of the fractures, which are represented as planar polygons, are used as the seed of 2D-nMAPS to produce a conforming Delaunay triangulation. The combined mesh of the DFN is used as the seed for 3D-nMAPS, which produces conforming Delaunay tetrahedra surrounding the network. Under a set of conditions that naturally arise in maximal Poisson-disk samples and are satisfied by nMAPS, the two-dimensional Delaunay triangulations are guaranteed to only have well-behaved triangular faces. While nMAPS does not provide triangulation quality bounds in more than two dimensions, we found that low-quality tetrahedra in 3D are infrequent, can be readily detected and removed, and a high quality balanced mesh is produced.</p>
  </div>

  <div class="pub-toggle hidden" id="bib-3">
    <pre>@article{KROTZ_2022_PoissonDiskDFN,
title = {Variable resolution Poisson-disk sampling for meshing discrete fracture networks},
journal = {Journal of Computational and Applied Mathematics},
volume = {407},
pages = {114094},
year = {2022},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2022.114094},
url = {https://www.sciencedirect.com/science/article/pii/S0377042722000073},
author = {Johannes Krotz and Matthew R. Sweeney and Carl W. Gable and Jeffrey D. Hyman and Juan M. Restrepo},
keywords = {Discrete fracture network, Maximal Poisson-disk sampling, Mesh generation, Conforming Delaunay triangulation},
abstract = {We present the near-Maximal Algorithm for Poisson-disk Sampling (nMAPS) to generate point 
        distributions for variable resolution Delaunay triangular and tetrahedral meshes in two and 
        three-dimensions, respectively. nMAPS consists of two principal stages. In the first stage, 
        an initial point distribution is produced using a cell-based rejection algorithm. In the 
        second stage, holes in the sample are detected using an efficient background grid and filled
        in to obtain a near-maximal covering. Extensive testing shows that nMAPS generates a variable 
        resolution mesh in linear run time with the number of accepted points. We demonstrate nMAPS 
        capabilities by meshing three-dimensional discrete fracture networks (DFN) and the surrounding 
        volume. The discretized boundaries of the fractures, which are represented as planar polygons, are 
        used as the seed of 2D-nMAPS to produce a conforming Delaunay triangulation. The combined mesh
        of the DFN is used as the seed for 3D-nMAPS, which produces conforming Delaunay tetrahedra 
        surrounding the network. Under a set of conditions that naturally arise in maximal Poisson-disk 
        samples and are satisfied by nMAPS, the two-dimensional Delaunay triangulations are guaranteed
        to only have well-behaved triangular faces. While nMAPS does not provide triangulation quality 
        bounds in more than two dimensions, we found that low-quality tetrahedra in 3D are infrequent, 
        can be readily detected and removed, and a high quality balanced mesh is produced.}}
   </pre>
  </div>

  <div class="pub-toggle hidden" id="sum-3">

  <p>
    When engineers and geoscientists model how water, oil, gas, or contaminants move underground, they often
    need to account for cracks in the rock. These fractures form intricate networks that guide where fluids can
    flow. To simulate this on a computer, you don’t just draw the fractures—you build a <strong>mesh</strong>:
    a set of triangles (in 2D) or tetrahedra (in 3D) that fills the fractures and the surrounding rock.
  </p>

  <p>
    There’s a balancing act. If the mesh is too coarse, you miss important details. If it’s too fine, the simulation
    can become prohibitively expensive. And if the mesh has poorly shaped elements, numerical solvers can struggle.
    This paper introduces <strong>nMAPS</strong> (near-Maximal Algorithm for Poisson-disk Sampling), a way to
    generate meshes that aim to be both efficient and well-balanced for discrete fracture networks (DFNs).
  </p>

  <h3>The Core Idea: Poisson-Disk Sampling</h3>
  <p>
    nMAPS starts by <em>sprinkling points</em> using <strong>Poisson-disk sampling</strong>. Think of scattering seeds
    so that no two land too close together (to avoid clusters) and the whole area is covered (to avoid gaps).
    nMAPS extends this with <strong>variable resolution</strong>: it places more points where fractures intersect, where
    geometry and gradients are complex, and fewer points farther away, where things are smoother.
  </p>

  <p>
    After placing points, nMAPS connects them with a <strong>Delaunay triangulation</strong> to make the mesh. In 2D,
    this produces triangles that are better shaped for simulation. In 3D, the method includes a practical step to
    <em>detect and remove “slivers”</em>, unhealthy, skinny tetrahedra, then re-triangulates, improving overall mesh quality.
  </p>

  <h3>Why It Matters</h3>
  <p>
    Reliable meshes are essential for trustworthy simulations of fractured rock. By directly generating a variable-resolution
    point set and then triangulating, nMAPS can reduce unnecessary computational effort while still capturing the features
    that matter. In tests on synthetic DFNs, the approach produced well-shaped elements in 2D and reduced problematic
    tetrahedra in 3D. It also compared favorably with existing workflows that refine meshes iteratively, with run times
    that scale efficiently with the number of points.
  </p>

  <p>
    This makes nMAPS a practical option for teams modeling groundwater flow, energy extraction, or contaminant transport
    in fractured media, and it may be useful in other settings that face similar meshing challenges.
  </p>

  <h3>In Summary</h3>
  <p>
    nMAPS offers a straightforward way to build meshes that balance accuracy, efficiency, and numerical robustness.
    It doesn’t solve every meshing challenge, but it provides a useful step toward making fracture-network simulations
    more dependable and accessible.
  </p>





</div>
    </div>

<!-- Template

<div class="pub-year">2023</div>

<div class="pub-entry">
  <div class="pub-info">
    <p class="pub-title">High-Order Methods for Neutron Transport</p>
    <p class="pub-authors"><u>Johannes Krotz</u>, Author B, Author C</p>
    <p class="pub-journal"><em>Annals of Nuclear Engineering</em></p>
  </div>

  <div class="pub-buttons">
    <button class="pub-btn toggle-btn" data-target="abs-2">Abstract</button>
    <button class="pub-btn toggle-btn" data-target="bib-2">BibTeX</button>
    <a class="pub-btn" href="https://journal.com/paper456" target="_blank">URL</a>
    <button class="pub-btn toggle-btn" data-target="sum-2">In Simple Terms</button>
  </div>

  <div class="pub-toggle hidden" id="abs-2">
    <p>This paper introduces a new high-order discretization method for neutron transport in multigroup settings with improved convergence guarantees.</p>
  </div>

  <div class="pub-toggle hidden" id="bib-2">
    <pre>@article{krotz2023neutron,
  title={High-Order Methods for Neutron Transport},
  author={Johannes Krotz and Author B and Author C},
  journal={Annals of Nuclear Engineering},
  year={2023}
}</pre>
  </div>

  <div class="pub-toggle hidden" id="sum-2">
    <p>Neutron transport helps us simulate nuclear reactors. This paper improves how these simulations are done, making them faster and more accurate.</p>
  </div>
</div>
-->



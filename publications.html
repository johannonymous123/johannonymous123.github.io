---
layout: default
title: Publications
---

<!-- Updated: 2025-08-29 -->
<h1>Publications</h1>

<p>
  Below is a list of my publications in reverse chronological order. For each publication, you can access the 
  <strong>Abstract</strong> for a technical summary, <strong>BibTeX</strong> for citation format, the 
  <strong>URL</strong> to view the full paper, and <strong>In Simple Terms</strong> for an accessible 
  explanation of the work and its significance.
</p>

<div class="publications">

  <div class="pub-year">2025</div>

  <div class="pub-entry">
    <div class="pub-info">
      <p class="pub-title">Curvilinear coordinates and curvature in radiative transport</p>
      <p class="pub-authors"><u>Johannes Krotz</u>, Ryan G. McClarren</p>
      <p class="pub-journal"><em>Journal of Computational and Theoretical Transport</em> (under review)</p>
    </div>

    <div class="pub-buttons">
      <button class="pub-btn toggle-btn" data-target="abs-4">Abstract</button>
      <button class="pub-btn toggle-btn" data-target="bib-4">BibTeX</button>
      <a class="pub-btn" href="https://doi.org/10.48550/arXiv.2508.20852" target="_blank">URL</a>
      <button class="pub-btn toggle-btn" data-target="sum-4">In Simple Terms</button>
    </div>

    <div class="pub-toggle hidden" id="abs-4">
      <p>We derive a general expression for the streaming term in radiative transport equations and other transport problems when formulated in curvilinear coordinates, emphasizing coordinate systems adapted to the geometry of the domain and the directional dependence of particle transport. By parametrizing the angular variable using a local orthonormal frame, we express directional derivatives in terms of curvature-related quantities that reflect the geometry of underlying spatial manifolds. Our formulation highlights how the interaction between coordinate choices and curvature influences the streaming operator, offering geometric interpretations of its components. The resulting framework offers intuitive insight into when and how angular dependence can be simplified and may guide the selection of coordinate systems that balance analytical tractability and computational efficiency.</p>
    </div>

    <div class="pub-toggle hidden" id="bib-4">
      <pre>@misc{krotz2025curvilinearcoordinatescurvatureradiative,
      title={Curvilinear coordinates and curvature in radiative transport}, 
      author={Johannes Krotz and Ryan G. McClarren},
      year={2025},
      eprint={2508.20852},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2508.20852}, 
}</pre>
    </div>

    <div class="pub-toggle hidden" id="sum-4">
  <p>
    When scientists simulate how light, radiation, or particles move through a medium — whether inside a star, in the
    Earth's atmosphere, or in a nuclear reactor — they use <strong>transport equations</strong>. At the heart of these
    equations is a deceptively simple piece: the <strong>streaming term</strong>, which tracks how particles move forward
    in space and direction.
  </p>

  <p>
    This sounds straightforward, but there's a twist. Particles don't always move through nice, flat, box-shaped domains.
    Often the geometry is curved — like spherical shells around a planet, cylindrical reactors, or more irregular shapes.
    When the geometry is curved, the math describing particle directions also bends. Choosing the right coordinate system
    can make the problem much easier — or much harder.
  </p>

  <h3>The Core Idea: Curvature Meets Transport</h3>
  <p>
    This work tackles the streaming term when the <em>coordinates</em> themselves are curved — so-called <strong>curvilinear
    coordinates</strong>. Instead of sticking with the usual Cartesian grid, we allow coordinates that better match the
    geometry of the problem (spheres, cylinders, ellipses, translating surfaces, etc.).
  </p>

  <p>
    The key contribution is a <strong>unified way</strong> to write the directional derivative (how particles stream) in
    terms of <strong>curvature quantities</strong> — intuitive geometric measures of how surfaces bend and how directions
    twist around each other. In plain terms:
  </p>
  <ul>
    <li>The math of particle transport isn't just about position and direction.</li>
    <li>It's also about how the <em>shape of space</em> (flat vs. curved) changes the apparent motion.</li>
    <li>By expressing the streaming operator via curvatures, we see how geometry and physics interact.</li>
  </ul>

  <h3>Why This Matters</h3>
  <p>
    Traditionally, deriving the streaming term in a curved system requires tedious, case-by-case geometry. Each new
    coordinate choice (spherical, cylindrical, elliptical…) brings its own messy formulas. This framework offers a
    <strong>common language</strong> based on curvature, which helps in three ways:
  </p>
  <ul>
    <li><strong>Simplification:</strong> it tells you when certain terms vanish in a given geometry, so the equations get simpler.</li>
    <li><strong>Intuition:</strong> instead of pages of indices, you reason with "is the surface flat, curved, or twisting?"</li>
    <li><strong>Practical guidance:</strong> it informs coordinate choices that balance tractability and accuracy.</li>
  </ul>

  <h3>Putting It Into Practice</h3>
  <p>
    The paper walks through familiar cases — cylindrical and spherical setups — and also more involved choices like ellipsoids
    or translating graphs. The general recipe shows:
  </p>
  <ul>
    <li><strong>Cylinders:</strong> several terms drop out because the relevant surfaces are flat and directions don't twist.</li>
    <li><strong>Spheres:</strong> curvature naturally appears with clean, symmetric contributions.</li>
    <li><strong>Ellipsoids / translating surfaces:</strong> complexity increases, but the curvature view still identifies which
      pieces matter and why.</li>
  </ul>

  <h3>Takeaway</h3>
  <p>
    The contribution isn't a new solver; it's a <strong>geometric perspective</strong> that makes derivations clearer and
    decisions more informed. By seeing transport through the lens of curvature, researchers can:
  </p>
  <ul>
    <li>avoid unnecessary terms,</li>
    <li>understand the effect of geometry on streaming, and</li>
    <li>choose coordinate systems that make the physics — and the computation — more manageable.</li>
  </ul>

  <p>
    It's a modest but useful step: connecting particle transport — a cornerstone of many physical simulations — more directly
    with the language of geometry.
  </p>
    </div>
  </div>

  <div class="pub-entry">
    <div class="pub-info">
      <p class="pub-title">A dynamic likelihood approach to filtering transport processes: advection-diffusion dynamics</p>
      <p class="pub-authors"> <u>Johannes Krotz</u>,Juan M. Restrepo, Jorge Ramirez</p>
      <p class="pub-journal"><em>Journal of Computational Physics</em></p>
    </div>

    <div class="pub-buttons">
      <button class="pub-btn toggle-btn" data-target="abs-1">Abstract</button>
      <button class="pub-btn toggle-btn" data-target="bib-1">BibTeX</button>
      <a class="pub-btn" href="https://doi.org/10.1016/j.jcp.2025.114089" target="_blank">URL</a>
     <button class="pub-btn toggle-btn" data-target="sum-1">In Simple Terms</button>
    </div>

    <div class="pub-toggle hidden" id="abs-1">
      <p>
      A Bayesian data assimilation scheme is formulated for advection-dominated advective and diffusive evolutionary problems, based upon the Dynamic Likelihood (DLF) approach to filtering. The DLF was developed specifically for hyperbolic problems –waves–, and in this paper, it is extended via a split step formulation, to handle advection-diffusion problems. In the dynamic likelihood approach, observations and their statistics are used to propagate probabilities along characteristics, evolving the likelihood in time. The estimate posterior thus inherits phase information. For advection-diffusion the advective part of the time evolution is handled on the basis of observations alone, while the diffusive part is informed through the model as well as observations. We expect, and indeed show here, that in advection-dominated problems, the DLF approach produces better estimates than other assimilation approaches, particularly when the observations are sparse and have low uncertainty. The added computational expense of the method is cubic in the total number of observations over time, which is on the same order of magnitude as a standard Kalman filter and can be mitigated by bounding the number of forward propagated observations, discarding the least informative data.</p>
    </div>

    <div class="pub-toggle hidden" id="bib-1">
      <pre>@article{KROTZ_2025_DynamicLikelihoodFilter,
title = {A dynamic likelihood approach to filtering transport processes: advection-diffusion dynamics},
journal = {Journal of Computational Physics},
volume = {536},
pages = {114089},
year = {2025},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2025.114089},
url = {https://www.sciencedirect.com/science/article/pii/S0021999125003729},
author = {Johannes Krotz and Juan M. Restrepo and Jorge Ramirez},
keywords = {Data assimilation, Bayesian estimation, Dynamic likelihood filter, 
Advection-diffusion, Transport, Kalman filter},
abstract = {A Bayesian data assimilation scheme is formulated for advection-dominated advective 
        and diffusive evolutionary problems, based upon the Dynamic Likelihood (DLF) approach to 
        filtering. The DLF was developed specifically for hyperbolic problems –waves–, and in this
        paper, it is extended via a split step formulation, to handle advection-diffusion problems. 
        In the dynamic likelihood approach, observations and their statistics are used to propagate 
        probabilities along characteristics, evolving the likelihood in time. The estimate posterior
        thus inherits phase information. For advection-diffusion the advective part of the time 
        evolution is handled on the basis of observations alone, while the diffusive part is informed 
        through the model as well as observations. We expect, and indeed show here, that in 
        advection-dominated problems, the DLF approach produces better estimates than other 
        assimilation approaches, particularly when the observations are sparse and have low 
        uncertainty. The added computational expense of the method is cubic in the total number of 
        observations over time, which is on the same order of magnitude as a standard Kalman filter 
        and can be mitigated by bounding the number of forward propagated observations, discarding 
        the least informative data.}}}
        </pre>
    </div>
<div class="pub-toggle hidden" id="sum-1">
  <p>
    Imagine you’re trying to follow the spread of smoke in the air, or how pollutants drift and mix in a river. These
    processes are driven by two forces: <strong>advection</strong> (movement along currents, carrying things downstream
    or with the wind) and <strong>diffusion</strong> (spreading and blurring out over time). Scientists want to predict
    these processes as accurately as possible — but the problem is, we never have perfect information. We rely on
    <em>models</em> (our best guess of the physics) and <em>observations</em> (data from sensors). Blending these two
    is called <strong>data assimilation</strong>.
  </p>

  <p>
    The classic tool for this job is the <strong>Kalman Filter</strong>, which acts like a smart balance: constantly
    correcting a model with incoming data. It works well when observations are plentiful and evenly spaced. But in the
    real world, data is often <em>sparse</em> (only a few sensors, not everywhere all the time) — and this is exactly
    where traditional methods begin to struggle.
  </p>

  <h3>The Core Idea: A Dynamic Likelihood</h3>
  <p>
    This paper develops the <strong>Dynamic Likelihood Filter (DLF)</strong> for advection–diffusion problems. The key
    innovation is that instead of waiting passively for the next measurement, the DLF <em>projects existing observations
    forward in time</em> along the natural flow of the system. In other words, it doesn’t just use a measurement once and
    discard it. It “carries” that information along the path particles or pollutants would naturally follow, generating
    <strong>pseudo-observations</strong> at new times and places where sensors may not exist.
  </p>

  <p>
    For the <em>advective part</em> of the process (the bulk flow), these pseudo-observations are especially powerful.
    For the <em>diffusive part</em> (the spreading and smoothing), the DLF blends in predictions from the model itself.
    Together, this allows the filter to maintain not only the <em>amount</em> of what is being transported, but also its
    <em>location and phase</em> — something traditional methods tend to lose quickly as diffusion blurs the signal.
  </p>

  <h3>Why This Matters</h3>
  <p>
    The DLF is designed for the kinds of real-world challenges that matter most:
  </p>
  <ul>
    <li><strong>Sparse but reliable data:</strong> a few high-quality sensors instead of dense networks.</li>
    <li><strong>Advection-dominated systems:</strong> where currents, winds, or flows move things much more than they diffuse.</li>
    <li><strong>Uncertain models:</strong> situations where we know the equations but don’t capture every detail of reality.</li>
  </ul>
  <p>
    These situations arise in <strong>meteorology</strong> (weather forecasting with limited stations), <strong>environmental
    monitoring</strong> (tracking contaminants in rivers or the atmosphere), and <strong>engineering</strong> (any process
    where materials or signals are transported through space and time).
  </p>

  <h3>What the Results Show</h3>
  <p>
    Numerical experiments in the paper show that the DLF consistently outperforms the standard Kalman Filter when
    advection dominates and data is sparse. It:
  </p>
  <ul>
    <li>Produces more accurate estimates of both <em>intensity</em> and <em>location</em>.</li>
    <li>Corrects errors in initial conditions more quickly.</li>
    <li>Handles uncertain or slightly wrong models better, staying anchored to reality.</li>
  </ul>
  <p>
    There is a tradeoff: the DLF requires more computation. But the paper demonstrates practical ways to manage this,
    such as discarding “stale” pseudo-observations once they no longer add value. This keeps the method efficient while
    retaining its accuracy advantage.
  </p>

  <h3>The Bigger Picture</h3>
  <p>
    Originally, the Dynamic Likelihood idea was built for wave problems. Extending it to advection–diffusion, one of
    the most common forms of transport in nature and engineering,  shows its broader potential. By projecting
    observations dynamically, the DLF squeezes more information out of every measurement. It makes data assimilation
    <strong>smarter, more resilient to sparse data, and better at tracking where things actually are</strong>.
  </p>

  <p>
    <em>In short:</em> The Dynamic Likelihood Filter lets us do more with less. It turns limited observations into a
    continuous source of information, improving forecasts in fields from weather and climate science to pollution
    monitoring and engineering transport problems.
  </p>
</div>
  </div>

<div class="pub-year">2024</div>

<div class="pub-entry">
  <div class="pub-info">
    <p class="pub-title">A hybrid Monte Carlo, discontinuous Galerkin method for linear kinetic transport equations</p>
    <p class="pub-authors"><u>Johannes Krotz</u>, Cory D. Hauck, Ryan G. McClarren</p>
    <p class="pub-journal"><em>Journal of Computational Physics</em></p>
  </div>

  <div class="pub-buttons">
    <button class="pub-btn toggle-btn" data-target="abs-2">Abstract</button>
    <button class="pub-btn toggle-btn" data-target="bib-2">BibTeX</button>
    <a class="pub-btn" href="https://doi.org/10.1016/j.jcp.2024.113253" target="_blank">URL</a>
    <button class="pub-btn toggle-btn" data-target="sum-2">In Simple Terms</button>
  </div>

  <div class="pub-toggle hidden" id="abs-2">
    <p>We present a hybrid method for time-dependent particle transport problems that combines Monte Carlo (MC) estimation with deterministic solutions based on discrete ordinates. For spatial discretizations, the MC algorithm computes a piecewise constant solution and the discrete ordinates use bilinear discontinuous finite elements. From the hybridization of the problem, the resulting problem solved by Monte Carlo is scattering free, resulting in a simple, efficient solution procedure. Between time steps, we use a projection approach to “relabel” collided particles as uncollided particles. From a series of standard 2-D Cartesian test problems we observe that our hybrid method has improved accuracy and reduction in computational complexity of approximately an order of magnitude relative to standard discrete ordinates solutions.</p>
  </div>

  <div class="pub-toggle hidden" id="bib-2">
    <pre>@article{KROTZ_2024_HybridMCDG,
title = {A hybrid Monte Carlo, discontinuous Galerkin method for linear kinetic transport equations},
journal = {Journal of Computational Physics},
volume = {514},
pages = {113253},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2024.113253},
url = {https://www.sciencedirect.com/science/article/pii/S0021999124005011},
author = {Johannes Krotz and Cory D. Hauck and Ryan G. McClarren},
keywords = {Hybrid stochastic-deterministic method, Monte Carlo, Kinetic equations, Particle transport},
abstract = {We present a hybrid method for time-dependent particle transport problems that combines
         Monte Carlo (MC) estimation with deterministic solutions based on discrete ordinates. For 
         spatial discretizations, the MC algorithm computes a piecewise constant solution and the 
         discrete ordinates use bilinear discontinuous finite elements. From the hybridization of 
         the problem, the resulting problem solved by Monte Carlo is scattering free, resulting in 
         a simple, efficient solution procedure. Between time steps, we use a projection approach to 
         “relabel” collided particles as uncollided particles. From a series of standard 2-D Cartesian
         test problems we observe that our hybrid method has improved accuracy and reduction in 
         computational complexity of approximately an order of magnitude relative to standard discrete
         ordinates solutions.}}
    </pre>
  </div>

  <div class="pub-toggle hidden" id="sum-2">  
  <p>
    Imagine trying to predict how sunlight travels through Earth’s atmosphere, how radiation moves inside a nuclear reactor, 
    or how heat from a laser interacts with a dense plasma. All of these processes involve <strong>particles moving, bouncing, 
    and sometimes getting absorbed</strong> as they travel through a medium. Modeling these processes accurately is a monumental challenge. 
  </p>

  <p>
    Traditionally, scientists have relied on two approaches: <em>deterministic methods</em>, which translate physics into large systems 
    of equations, and <em>Monte Carlo methods</em>, which simulate particle paths randomly. Each has strengths and weaknesses. Deterministic 
    methods can be precise but become prohibitively expensive at high resolution. Monte Carlo is flexible and natural for complex geometries, 
    but it’s noisy and often requires an enormous number of samples to be reliable. 
  </p>

  <h3>The Core Idea: Pre- and Post-Collision Particles</h3>
  <p>
    The hybrid method presented in this paper blends the two approaches by recognizing that particles behave differently depending 
    on whether they’ve collided yet. 
  </p>
  <ul>
    <li><strong>Pre-collision (uncollided):</strong> particles stream like rays of light. Their paths are straightforward, and Monte Carlo 
    can handle them efficiently — essentially like ray tracing.</li>
    <li><strong>Post-collision (collided):</strong> once particles scatter, their distribution becomes smoother and more diffuse. A deterministic 
    solver is better suited here, capturing the averaged behavior without tracking every random path.</li>
  </ul>
  <p>
    A clever <em>relabeling step</em> keeps the balance by periodically shifting some particles back into the pre-collision group, ensuring 
    the method stays both accurate and efficient over time.
  </p>

  <h3>Results &amp; Takeaways</h3>
  <p>
    Across standard benchmark problems, the hybrid method consistently delivered <strong>greater accuracy at significantly lower cost</strong>. 
    Unlike pure deterministic solvers, it avoided artificial streaks (“ray effects”), and unlike pure Monte Carlo, it didn’t require massive 
    sample counts. Overall, it achieved comparable or better accuracy with <em>about an order of magnitude less computational complexity</em>.
  </p>
  <p>
    This is more than a technical curiosity, it has direct relevance to <strong>nuclear engineering</strong>, <strong>astrophysics</strong>, 
    <strong>medical physics</strong>, and <strong>fusion research</strong>. Anywhere particles stream and scatter through matter, this method 
    can provide faster, cleaner, and more reliable simulations. 
  </p>

  <h3>Looking Ahead</h3>
  <p>
    While the current work focuses on single-energy problems, the approach naturally extends to more complex cases, such as multi-energy 
    neutron transport or nonlinear radiative transfer in astrophysics. Future versions may even include adaptive strategies to automatically 
    balance the two solvers as the simulation evolves. 
  </p>

  <p>
    <em>In short:</em> By treating particles before and after collisions differently,  Monte Carlo for the sharp, ray-like stage, and deterministic 
    solvers for the scattered stage,  this hybrid method achieves what neither approach can do alone: <strong>faster, more accurate, 
    and more practical simulations of particle transport</strong>.
  </p>
  </div>
</div>




<div class="pub-year">2023</div>

<div class="pub-entry">
  <div class="pub-info">
    <p class="pub-title">Variable resolution Poisson-disk sampling for meshing discrete fracture networks</p>
    <p class="pub-authors"><u>Johannes Krotz</u>, Matthew R. Sweeney, Carl W. Gable, Jeffrey D. Hyman, Juan M. Restrepo </p>
    <p class="pub-journal"><em>Journal of Computational and Applied Mathematics</em></p>
  </div>

  <div class="pub-buttons">
    <button class="pub-btn toggle-btn" data-target="abs-3">Abstract</button>
    <button class="pub-btn toggle-btn" data-target="bib-3">BibTeX</button>
    <a class="pub-btn" href="https://doi.org/10.1016/j.cam.2022.114094" target="_blank">URL</a>
    <button class="pub-btn toggle-btn" data-target="sum-3">In Simple Terms</button>
  </div>

  <div class="pub-toggle hidden" id="abs-3">
    <p>We present the near-Maximal Algorithm for Poisson-disk Sampling (nMAPS) to generate point distributions for variable resolution Delaunay triangular and tetrahedral meshes in two and three-dimensions, respectively. nMAPS consists of two principal stages. In the first stage, an initial point distribution is produced using a cell-based rejection algorithm. In the second stage, holes in the sample are detected using an efficient background grid and filled in to obtain a near-maximal covering. Extensive testing shows that nMAPS generates a variable resolution mesh in linear run time with the number of accepted points. We demonstrate nMAPS capabilities by meshing three-dimensional discrete fracture networks (DFN) and the surrounding volume. The discretized boundaries of the fractures, which are represented as planar polygons, are used as the seed of 2D-nMAPS to produce a conforming Delaunay triangulation. The combined mesh of the DFN is used as the seed for 3D-nMAPS, which produces conforming Delaunay tetrahedra surrounding the network. Under a set of conditions that naturally arise in maximal Poisson-disk samples and are satisfied by nMAPS, the two-dimensional Delaunay triangulations are guaranteed to only have well-behaved triangular faces. While nMAPS does not provide triangulation quality bounds in more than two dimensions, we found that low-quality tetrahedra in 3D are infrequent, can be readily detected and removed, and a high quality balanced mesh is produced.</p>
  </div>

  <div class="pub-toggle hidden" id="bib-3">
    <pre>@article{KROTZ_2022_PoissonDiskDFN,
title = {Variable resolution Poisson-disk sampling for meshing discrete fracture networks},
journal = {Journal of Computational and Applied Mathematics},
volume = {407},
pages = {114094},
year = {2022},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2022.114094},
url = {https://www.sciencedirect.com/science/article/pii/S0377042722000073},
author = {Johannes Krotz and Matthew R. Sweeney and Carl W. Gable and Jeffrey D. Hyman and Juan M. Restrepo},
keywords = {Discrete fracture network, Maximal Poisson-disk sampling, Mesh generation, Conforming Delaunay triangulation},
abstract = {We present the near-Maximal Algorithm for Poisson-disk Sampling (nMAPS) to generate point 
        distributions for variable resolution Delaunay triangular and tetrahedral meshes in two and 
        three-dimensions, respectively. nMAPS consists of two principal stages. In the first stage, 
        an initial point distribution is produced using a cell-based rejection algorithm. In the 
        second stage, holes in the sample are detected using an efficient background grid and filled
        in to obtain a near-maximal covering. Extensive testing shows that nMAPS generates a variable 
        resolution mesh in linear run time with the number of accepted points. We demonstrate nMAPS 
        capabilities by meshing three-dimensional discrete fracture networks (DFN) and the surrounding 
        volume. The discretized boundaries of the fractures, which are represented as planar polygons, are 
        used as the seed of 2D-nMAPS to produce a conforming Delaunay triangulation. The combined mesh
        of the DFN is used as the seed for 3D-nMAPS, which produces conforming Delaunay tetrahedra 
        surrounding the network. Under a set of conditions that naturally arise in maximal Poisson-disk 
        samples and are satisfied by nMAPS, the two-dimensional Delaunay triangulations are guaranteed
        to only have well-behaved triangular faces. While nMAPS does not provide triangulation quality 
        bounds in more than two dimensions, we found that low-quality tetrahedra in 3D are infrequent, 
        can be readily detected and removed, and a high quality balanced mesh is produced.}}
   </pre>
  </div>

  <div class="pub-toggle hidden" id="sum-3">

  <p>
    When engineers and geoscientists model how water, oil, gas, or contaminants move underground, they often
    need to account for cracks in the rock. These fractures form intricate networks that guide where fluids can
    flow. To simulate this on a computer, you don’t just draw the fractures—you build a <strong>mesh</strong>:
    a set of triangles (in 2D) or tetrahedra (in 3D) that fills the fractures and the surrounding rock.
  </p>

  <p>
    There’s a balancing act. If the mesh is too coarse, you miss important details. If it’s too fine, the simulation
    can become prohibitively expensive. And if the mesh has poorly shaped elements, numerical solvers can struggle.
    This paper introduces <strong>nMAPS</strong> (near-Maximal Algorithm for Poisson-disk Sampling), a way to
    generate meshes that aim to be both efficient and well-balanced for discrete fracture networks (DFNs).
  </p>

  <h3>The Core Idea: Poisson-Disk Sampling</h3>
  <p>
    nMAPS starts by <em>sprinkling points</em> using <strong>Poisson-disk sampling</strong>. Think of scattering seeds
    so that no two land too close together (to avoid clusters) and the whole area is covered (to avoid gaps).
    nMAPS extends this with <strong>variable resolution</strong>: it places more points where fractures intersect, where
    geometry and gradients are complex, and fewer points farther away, where things are smoother.
  </p>

  <p>
    After placing points, nMAPS connects them with a <strong>Delaunay triangulation</strong> to make the mesh. In 2D,
    this produces triangles that are better shaped for simulation. In 3D, the method includes a practical step to
    <em>detect and remove “slivers”</em>, unhealthy, skinny tetrahedra, then re-triangulates, improving overall mesh quality.
  </p>

  <h3>Why It Matters</h3>
  <p>
    Reliable meshes are essential for trustworthy simulations of fractured rock. By directly generating a variable-resolution
    point set and then triangulating, nMAPS can reduce unnecessary computational effort while still capturing the features
    that matter. In tests on synthetic DFNs, the approach produced well-shaped elements in 2D and reduced problematic
    tetrahedra in 3D. It also compared favorably with existing workflows that refine meshes iteratively, with run times
    that scale efficiently with the number of points.
  </p>

  <p>
    This makes nMAPS a practical option for teams modeling groundwater flow, energy extraction, or contaminant transport
    in fractured media, and it may be useful in other settings that face similar meshing challenges.
  </p>

  <h3>In Summary</h3>
  <p>
    nMAPS offers a straightforward way to build meshes that balance accuracy, efficiency, and numerical robustness.
    It doesn’t solve every meshing challenge, but it provides a useful step toward making fracture-network simulations
    more dependable and accessible.
  </p>





</div>
    </div>

<!-- Template

<div class="pub-year">2023</div>

<div class="pub-entry">
  <div class="pub-info">
    <p class="pub-title">High-Order Methods for Neutron Transport</p>
    <p class="pub-authors"><u>Johannes Krotz</u>, Author B, Author C</p>
    <p class="pub-journal"><em>Annals of Nuclear Engineering</em></p>
  </div>

  <div class="pub-buttons">
    <button class="pub-btn toggle-btn" data-target="abs-2">Abstract</button>
    <button class="pub-btn toggle-btn" data-target="bib-2">BibTeX</button>
    <a class="pub-btn" href="https://journal.com/paper456" target="_blank">URL</a>
    <button class="pub-btn toggle-btn" data-target="sum-2">In Simple Terms</button>
  </div>

  <div class="pub-toggle hidden" id="abs-2">
    <p>This paper introduces a new high-order discretization method for neutron transport in multigroup settings with improved convergence guarantees.</p>
  </div>

  <div class="pub-toggle hidden" id="bib-2">
    <pre>@article{krotz2023neutron,
  title={High-Order Methods for Neutron Transport},
  author={Johannes Krotz and Author B and Author C},
  journal={Annals of Nuclear Engineering},
  year={2023}
}</pre>
  </div>

  <div class="pub-toggle hidden" id="sum-2">
    <p>Neutron transport helps us simulate nuclear reactors. This paper improves how these simulations are done, making them faster and more accurate.</p>
  </div>
</div>
-->

<h1>Theses and Dissertation</h1>

<div class="publications">

  <div class="pub-year">2024</div>

  <div class="pub-entry">
    <div class="pub-info">
      <p class="pub-title">Probabilistic and data-driven methods for numerical PDEs</p>
      <p class="pub-authors"><u>Johannes Krotz</u></p>
      <p class="pub-journal"><em>Dissertation</em></p>
    </div>

    <div class="pub-buttons">
      <button class="pub-btn toggle-btn" data-target="abs-diss-2024">Abstract</button>
      <button class="pub-btn toggle-btn" data-target="bib-diss-2024">BibTeX</button>
      <a class="pub-btn" href="{{ '/assets/Dissertation_Johannes.pdf' | relative_url }}" target="_blank">PDF</a>
      <button class="pub-btn toggle-btn" data-target="sum-diss-2024">In Simple Terms</button>
    </div>

    <div class="pub-toggle hidden" id="abs-diss-2024">
      <p>This dissertation consists of three integral self-contained parts. The first part develops a novel Monte Carlo algorithm, called the near-Maximal Algorithm for Poisson-disk Sampling (nMAPS), to efficiently generate the nodes of a high-quality mesh for the calculation of flow and the associated transport of chemical species in low-permeability fractured rock, such as shale and granite. A good mesh balances accuracy requirements with a reasonable computational cost, i.e., it is generated efficiently, dense where necessary for accuracy, and contains no cells that cause instabilities or blown-up errors. Quality bounds for meshes generated through nMAPS are proven, and its efficiency is demonstrated through numerical experiments (see <em>Variable resolution Poisson-disk sampling</em> paper above for complete details).</p>
      <p>In the second part, a deterministic Monte Carlo hybrid method for time-dependent problems based on the physics of particle transport described through the linear Boltzmann equation is presented. The method splits the system into collided and uncollided particles and treats these sets with different methods. Uncollided particles are handled through high-accuracy Monte Carlo methods, while the density of collided particles is calculated using discontinuous Galerkin methods. Theoretical details of the algorithm are developed and shown to be effective through numerical experiments. The properties associated with the labeling as collided and uncollided leverage the respective strengths of these methods, allowing for overall more accurate and computationally efficient solving than each method on its own (see <em>Hybrid Monte Carlo, discontinuous Galerkin method</em> paper above for complete details).</p>
      <p>In the last chapter, an extension to the Dynamic Likelihood Filter (DLF) is presented to include Advection-Diffusion equations. The DLF is a Bayesian estimation method specifically designed for wave-related problems. It improves on traditional methods, such as variants of Kalman filters, by not only using data at its time of observation but also at later times by propagating observations forward through time. This enriches the available data and improves predictions and uncertainties. The theory to include diffusion in the framework of the DLF is developed, and it is shown through numerical experiments that the DLF outperforms traditional data assimilation techniques, especially when observations are precise but sparse in space and time (see <em>Dynamic likelihood approach to filtering</em> paper above for complete details).</p>
    </div>

    <div class="pub-toggle hidden" id="bib-diss-2024">
      <pre>@phdthesis{krotz2024dissertation,
  author = {Johannes Krotz},
  title = {Probabilistic and data-driven methods for numerical PDEs},
  school = {University of Tennessee, Knoxville},
  year = {2024},
  type = {Ph.D. dissertation}
}</pre>
    </div>

    <div class="pub-toggle hidden" id="sum-diss-2024">
      <p>This dissertation brings together three innovative computational methods that blend probability with traditional numerical approaches to solve challenging problems in science and engineering. Rather than developing entirely new theory from scratch, it demonstrates how <strong>smart combinations of existing techniques</strong> can lead to significant improvements in computational performance and reliability.</p>
      
      <h3>Three Connected Innovations</h3>
      <p>The dissertation consists of three main chapters, each corresponding to a published paper:</p>
      <ul>
        <li><strong>Chapter 2:</strong> Developing better ways to create computational meshes for fractured rock systems (see the <em>Variable resolution Poisson-disk sampling</em> paper above for details)</li>
        <li><strong>Chapter 3:</strong> Improving how we combine computer models with real-world data to track pollution and other transported substances (see the <em>Dynamic likelihood approach to filtering</em> paper above for details)</li>
        <li><strong>Chapter 4:</strong> Creating hybrid solvers that combine random sampling with deterministic methods for radiation transport problems (see the <em>Hybrid Monte Carlo, discontinuous Galerkin method</em> paper above for details)</li>
      </ul>

      <h3>The Common Thread: Probabilistic Meets Deterministic</h3>
      <p>While these three projects tackle different applications — underground fluid flow, environmental monitoring, and radiation transport — they share a common philosophy: <strong>combining the flexibility of probabilistic methods with the precision of deterministic approaches</strong>. This hybrid thinking allows each method to capture the best of both worlds while avoiding their individual weaknesses.</p>

      <h3>Real-World Impact</h3>
      <p>The dissertation demonstrates that computational science doesn't always need revolutionary breakthroughs. Sometimes, <strong>thoughtful integration of existing tools</strong> can solve practical problems more effectively. Each method has been tested on realistic problems and shown to outperform traditional approaches, making complex simulations more accessible and reliable for engineers and scientists.</p>

      <p><em>In essence:</em> This work shows how probabilistic thinking can enhance traditional computational methods, leading to faster, more accurate, and more robust simulations across diverse fields in science and engineering.</p>
    </div>
  </div>

  <div class="pub-year">2019</div>

  <div class="pub-entry">
    <div class="pub-info">
      <p class="pub-title">Computer Simulation of Gel Formation in Colloidal Systems of Sticky Rods </p>
      <p class="pub-authors"><u>Johannes Krotz</u></p>
      <p class="pub-journal"><em>Master's Thesis</em></p>
    </div>

    <div class="pub-buttons">
      <button class="pub-btn toggle-btn" data-target="abs-master-2019">Abstract</button>
      <button class="pub-btn toggle-btn" data-target="bib-master-2019">BibTeX</button>
      <a class="pub-btn" href="{{ '/assets/Master.pdf' | relative_url }}" target="_blank">PDF</a>
      <button class="pub-btn toggle-btn" data-target="sum-master-2019">In Simple Terms</button>
    </div>

    <div class="pub-toggle hidden" id="abs-master-2019">
      <p>We develop and validate a simulation framework for colloidal gelation. We first reproduce the benchmark results of Santos, Campanella, and Carignano for spherical, gel-forming particles, then extend the methodology to more complex systems of "sticky" spherocylindrical rods interacting via a Kihara-like potential. Using comprehensive parameter sweeps documented for reproducibility, we analyze the emergence of porous, percolating networks and conduct a topological characterization of the resulting microstructures. This characterization leverages Early TDA to extract multiscale connectivity features and to define topology-driven metrics for automated comparison between simulations and experiments. Our simulations reveal a clear dependence of network formation on rod aspect ratio and particle density, consistent with established theory and, to our knowledge, not previously demonstrated for spherocylindrical colloids with Kihara-type interactions. Rheological probing of the simulated systems shows signatures characteristic of gels, which supports the structural analysis. We further compare our computational results with experimental data obtained on Bastian Trepka's gels collected by Jacob Steindl. Although these first comparisons indicate that the present model is not yet sufficient to quantitatively describe those specific gelled systems, the agreement in qualitative trends and the robustness of our tools suggest strong potential. Overall, the work demonstrates functional, extensible methods for simulating gelation in rod-based colloids, provides topological data analysis based metrics that can aid automated comparison between experiments and simulations, and outlines several promising directions for future refinement and application.</p>
    </div>

    <div class="pub-toggle hidden" id="bib-master-2019">
      <pre>@mastersthesis{krotz2019master,
  author = {Johannes Krotz},
  title = {Computer Simulation of Gel Formation in Colloidal Systems of Sticky Rods},
  school = {Universität Konstanz},
  year = {2019},
  type = {Master's thesis}
}</pre>
    </div>

    <div class="pub-toggle hidden" id="sum-master-2019">
      <p>TBD</p>
    </div>
  </div>

  <div class="pub-year">2018</div>

  <div class="pub-entry">
    <div class="pub-info">
      <p class="pub-title">Gibt es Rotationssymmetrische Fische?</p>
      <p class="pub-authors"><u>Johannes Krotz</u></p>
      <p class="pub-journal"><em>Bachelor's Thesis (Mathematics)</em></p>
    </div>

    <div class="pub-buttons">
      <button class="pub-btn toggle-btn" data-target="abs-bachelor-2018">Abstract</button>
      <button class="pub-btn toggle-btn" data-target="bib-bachelor-2018">BibTeX</button>
      <a class="pub-btn" href="{{ '/assets/BAmathe.pdf' | relative_url }}" target="_blank">PDF</a>
      <button class="pub-btn toggle-btn" data-target="sum-bachelor-2018">In Simple Terms</button>
    </div>

    <div class="pub-toggle hidden" id="abs-bachelor-2018">
      <p>This bachelor's thesis investigates the existence of rotationally symmetric, homothetically shrinking "fish"-shaped hypersurface networks evolving by the Gaussian curvature flow. Building on prior work that established one-dimensional, lens/fish-shaped networks under curve-shortening/curvature flows, the thesis formulates the rotationally symmetric problem in R^{n+1} and derives three equivalent ordinary differential equations from the geometric PDE using distinct parameterizations (axial graph, angular, and radial graph). The core result is an alternative proof of existence in the one-dimensional case; the higher-dimensional case remains open, though supporting lemmas and numerical experiments suggest feasibility.</p>
      <p>The analysis includes (i) equivalence of the derived ODEs and preservation of geometric quantities under reparametrization, (ii) local existence and regularity near the rotation axis, (iii) comparison and barrier arguments to control curvature and intersection behavior with rays from the origin, and (iv) a shooting/mirror construction that enforces the 120 degree junction and orthogonality conditions required at the triple point of the fish network. Numerical solutions illustrate the shape and support the analytical construction. Overall, the work advances understanding of self-similar shrinkers for Gaussian curvature flow in rotational symmetry, fully resolving n=1 and outlining a pathway for n>1.</p>
    </div>

    <div class="pub-toggle hidden" id="bib-bachelor-2018">
      <pre>@bachelorsthesis{krotz2018bachelor,
  author = {Johannes Krotz},
  title = {Gibt es Rotationssymmetrische Fische?},
  school = {Universität Konstanz},
  year = {2018},
  type = {Bachelor's thesis},
  note = {Mathematics}
}</pre>
    </div>

    <div class="pub-toggle hidden" id="sum-bachelor-2018">
      <p>TBD</p>
    </div>
  </div>

  <div class="pub-year">2015</div>

  <div class="pub-entry">
    <div class="pub-info">
      <p class="pub-title">Quantum Transport in Topological Insulators - Superconductor Heterostructures</p>
      <p class="pub-authors"><u>Johannes Krotz</u></p>
      <p class="pub-journal"><em>Bachelor's Thesis (Physics)</em></p>
    </div>

    <div class="pub-buttons">
      <button class="pub-btn toggle-btn" data-target="abs-bachelor-2015">Abstract</button>
      <button class="pub-btn toggle-btn" data-target="bib-bachelor-2015">BibTeX</button>
      <a class="pub-btn" href="{{ '/assets/Bachelor_Physik.pdf' | relative_url }}" target="_blank">PDF</a>
      <button class="pub-btn toggle-btn" data-target="sum-bachelor-2015">In Simple Terms</button>
    </div>

    <div class="pub-toggle hidden" id="abs-bachelor-2015">
      <p>This thesis investigates quantum transport in two-dimensional time-reversal-invariant topological insulators, focusing on HgTe/CdTe quantum wells described by the Bernevig--Hughes--Zhang (BHZ) model. After reproducing the helical edge states protected against elastic backscattering, I derive an effective one-dimensional edge Hamiltonian and analyze how a localized magnetic (Zeeman) barrier breaks time-reversal symmetry, opens a gap in the edge spectrum, and enables controllable backscattering. In the low-energy limit, the resulting transmission and reflection yield a characteristic conductance suppression set by barrier strength and width.</p>
      <p>Placing an <em>s</em>-wave superconductor in proximity to the edge, I project the full Bogoliubov--de Gennes description onto the edge subspace and show that subgap transport is dominated by perfect Andreev reflection at an NS interface and by phase-dependent Andreev bound states in an SNS junction with energies <em>E</em> = |<em>α</em>| cos(<em>φ</em>/2). The corresponding Josephson current exhibits the expected current--phase relation and, under a perpendicular magnetic field, distinct interference patterns that differentiate edge-dominated from bulk-dominated transport.</p>
      <p>Finally, I propose transport protocols to extract microscopically the subband <em>g</em>-factors (<em>g</em><sub>E</sub>, <em>g</em><sub>H</sub>) and superconducting order parameters (<em>Δ</em><sub>E</sub>, <em>Δ</em><sub>H</sub>) from their effective edge quantities <em>ĝ</em> and <em>α</em>. By exploiting their linear dependence on a mixing parameter <em>γ</em> (set by BHZ parameters and sample thickness), measurements across devices with varied well thickness can determine these band-resolved couplings.</p>
    </div>

    <div class="pub-toggle hidden" id="bib-bachelor-2015">
      <pre>@bachelorsthesis{krotz2015bachelor,
  author = {Johannes Krotz},
  title = {Quantum Transport in Topological Insulators - Superconductor Heterostructures},
  school = {Universität Konstanz},
  year = {2015},
  type = {Bachelor's thesis},
  note = {Physics}
}</pre>
    </div>

    <div class="pub-toggle hidden" id="sum-bachelor-2015">
      <p>TBD</p>
    </div>
  </div>

</div>


